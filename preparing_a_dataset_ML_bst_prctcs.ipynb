{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing & Splitting — Full Breakdown\n",
    "\n",
    "## Overview 📊\n",
    "\n",
    "When preparing data for machine learning, especially clinical or bio data, it's critical to handle preprocessing steps correctly to avoid **data leakage** and ensure your model generalizes well.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps\n",
    "\n",
    "### 1 Drop missing values\n",
    "\n",
    "We remove rows with missing `gene_expression` to avoid contaminating model inputs.\n",
    "* if too many are dropped we may need to consider imputation if we will lose too much data.\n",
    "\n",
    "```\n",
    "df_clean = df.dropna(subset=['gene_expression'])\n",
    "```\n",
    "### 2 Split into features and target\n",
    "\n",
    "Separate features (`X`) from target (`y`).\n",
    "\n",
    "```\n",
    "X = df_clean[['gene_expression', 'age']]\n",
    "y = df_clean['response']\n",
    "```\n",
    "\n",
    "### 3 Train-test split with stratification\n",
    "We use train_test_split to split data into training and testing sets.\n",
    "\n",
    "* **Stratify**: Ensures that both sets maintain the same proportion of classes as the original data.\n",
    "\n",
    "* **Random_state**: Guarantees reproducibility of the split.\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "### Why stratify?\n",
    "\n",
    "When dealing with imbalanced datasets (like many clinical studies), class distributions can easily get skewed if you split randomly.\n",
    "\n",
    "For example, suppose your original dataset looks like this:\n",
    "\n",
    "| Response | Count | Percentage |\n",
    "|-----------|-------|------------|\n",
    "| 0         | 90    | 90%        |\n",
    "| 1         | 10    | 10%        |\n",
    "\n",
    "If you do a random split **without stratification**, you might accidentally end up with:\n",
    "\n",
    "- Training set: 95% class 0, 5% class 1\n",
    "- Test set: 85% class 0, 15% class 1\n",
    "\n",
    "This imbalance can mess up your model's ability to learn minority classes and skew evaluation metrics.\n",
    "\n",
    "When you use `stratify=y`, you preserve the original class distribution:\n",
    "\n",
    "| Response | Original % | After Stratify (Train/Test) % |\n",
    "|-----------|------------|-------------------------------|\n",
    "| 0         | 90%       | ~90%                        |\n",
    "| 1         | 10%       | ~10%                        |\n",
    "\n",
    "---\n",
    "\n",
    "**Key point:**\n",
    "> \"Stratification ensures both train and test sets reflect the true class distribution, so your model can generalize better and evaluation is fair.\"\n",
    "\n",
    "### 4 Standardize features\n",
    "We fit the scaler on X_train only to learn its mean and standard deviation, and then transform both train and test sets using these parameters.\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[['gene_expression']])\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled['gene_expression'] = scaler.transform(X_train[['gene_expression']])\n",
    "X_test_scaled['gene_expression'] = scaler.transform(X_test[['gene_expression']])\n",
    "```\n",
    "\n",
    "### ⚖️ Fit vs Transform\n",
    "\n",
    "| Step         | Purpose                                       | Applied to    | Mean ≈ 0, SD ≈ 1?                 |\n",
    "|---------------|-----------------------------------------------|---------------|----------------------------------|\n",
    "| `fit()`      | Learn mean and std from training data only    | Train data    | No change yet (just stores stats) |\n",
    "| `transform()`| Apply learned stats to scale data            | Train & Test | Yes for train, approx. for test |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🌋Knowledge bombs\n",
    "\n",
    "- **Why only `fit()` on train?**\n",
    "  > We only learn scaling parameters (mean and std) from training data to prevent data leakage. If you include test data when fitting, you let the model \"peek\" at future data, which inflates performance and ruins real-world validity.\n",
    "\n",
    "- **Why `transform()` on both?**\n",
    "  > Once trained on train data, the scaler should apply the exact same transformation to any new data (test or future real-world samples). This ensures consistency.\n",
    "\n",
    "- **Why test data ≠ exact zero mean & unit std?**\n",
    "  > Test data is transformed using the train stats, so it usually ends up *close* to zero mean and unit std, but not exactly, since its distribution might differ slightly.\n",
    "\n",
    "---\n",
    "\n",
    "#### Analogy\n",
    "\n",
    "> \"Fit is like learning the recipe in your own kitchen. Transform is applying that same recipe to your friend's food — no adjustments allowed. Never fit on test, or you basically sneak a taste and tweak their plate, and that’s cheating.\"\n",
    "\n",
    "---\n",
    "\n",
    "**One-liner mantra:**\n",
    "> \"Fit on train, transform on train, transform on test — never fit on test.\"\n",
    "\n",
    "### ☠️Data Leakage\n",
    "* We **do not** fit on test data to prevent leakage of future information into training.\n",
    "\n",
    "* Transforming test data uses only parameters from training data.\n",
    "\n",
    "### Clinical context\n",
    "* In clinical data, preserving true data distribution and preventing leakage is crucial for:\n",
    "\n",
    "* Accurate model evaluation\n",
    "\n",
    "* Regulatory compliance (Remember: on AWS 'Config is Code for Compliance')\n",
    "\n",
    "* Generalization to real-world cohorts\n",
    "\n",
    "### Analogy\n",
    "* **Fit on train**: Like tasting your sauce at home to get the seasoning right.\n",
    "\n",
    "* **Transform on train**: Apply that balance to your own dishes.\n",
    "\n",
    "* **Transform on test**: You use your sauce on a guest dish — but their dish might taste a bit different since you can't adjust it further.\n",
    "\n",
    "* **Never fit on test**: Don't taste and fix guest's dish at their table — that's cheating.\n",
    "\n",
    "### Summary mantra\n",
    "> \"Fit on train. Transform on train. Transform on test. Never fit on test. Stratify to keep class balance.\"\n",
    "\n"
   ],
   "id": "5338222be8596070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "?## Let's make some mock data called\n",
    "`mock_patients.csv`"
   ],
   "id": "cbf5ad7b1b8623c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-01T22:58:37.111820Z",
     "start_time": "2025-07-01T22:58:37.107630Z"
    }
   },
   "source": [
    "data = \"\"\"patient_id,gene_expression,age,response\n",
    "1,2.5,45,0\n",
    "2,3.2,52,1\n",
    "3,1.5,39,0\n",
    "4,2.9,61,0\n",
    "5,3.8,47,1\n",
    "6,,50,0\n",
    "7,2.2,55,0\n",
    "8,4.1,62,1\n",
    "9,3.0,48,0\n",
    "10,1.8,44,0\n",
    "\"\"\"\n",
    "\n",
    "with open(\"mock_patients.csv\", \"w\") as f:\n",
    "    f.write(data)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:25:57.929875Z",
     "start_time": "2025-07-01T23:25:56.343783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"mock_patients.csv\")\n",
    "df.head()"
   ],
   "id": "f1e7c083628c7110",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   patient_id  gene_expression  age  response\n",
       "0           1              2.5   45         0\n",
       "1           2              3.2   52         1\n",
       "2           3              1.5   39         0\n",
       "3           4              2.9   61         0\n",
       "4           5              3.8   47         1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>gene_expression</th>\n",
       "      <th>age</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read CSV & run our pipeline",
   "id": "e67daa2378f4b428"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:26:12.023053Z",
     "start_time": "2025-07-01T23:26:09.651495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"mock_patients.csv\")\n",
    "\n",
    "# Drop missing\n",
    "df_clean = df.dropna(subset=['gene_expression'])\n",
    "\n",
    "# Split X and y\n",
    "X = df_clean[['gene_expression', 'age']]\n",
    "y = df_clean['response']\n",
    "\n",
    "# Split with stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit scaler on train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[['gene_expression']])\n",
    "\n",
    "# Transform\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled['gene_expression'] = scaler.transform(X_train[['gene_expression']])\n",
    "X_test_scaled['gene_expression'] = scaler.transform(X_test[['gene_expression']])\n",
    "\n",
    "# Check means and stds\n",
    "print(\"Train gene_expression mean:\", X_train_scaled['gene_expression'].mean())\n",
    "print(\"Train gene_expression std:\", X_train_scaled['gene_expression'].std())\n",
    "\n",
    "print(\"Test gene_expression mean:\", X_test_scaled['gene_expression'].mean())\n",
    "print(\"Test gene_expression std:\", X_test_scaled['gene_expression'].std())\n",
    "\n",
    "print(\"\\nX_train_scaled:\")\n",
    "print(X_train_scaled)\n",
    "\n",
    "print(\"\\nX_test_scaled:\")\n",
    "print(X_test_scaled)\n"
   ],
   "id": "849b0953043bf1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train gene_expression mean: -3.806478941571965e-16\n",
      "Train gene_expression std: 1.0801234497346435\n",
      "Test gene_expression mean: -0.11261065411536281\n",
      "Test gene_expression std: 0.7962775715882578\n",
      "\n",
      "X_train_scaled:\n",
      "   gene_expression  age\n",
      "4         1.126107   47\n",
      "0        -0.337832   45\n",
      "3         0.112611   61\n",
      "8         0.225221   48\n",
      "9        -1.126107   44\n",
      "2        -1.463939   39\n",
      "7         1.463939   62\n",
      "\n",
      "X_test_scaled:\n",
      "   gene_expression  age\n",
      "6        -0.675664   55\n",
      "1         0.450443   52\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ⚖️ Handling Class Imbalance (On our \"Dummy-Data\"\n",
    "\n",
    "## Problem\n",
    "\n",
    "When one class dominates (e.g., 90% non-responders, 10% responders), the model might just always predict the majority class to get high \"accuracy,\" but fail to actually detect the minority (important) class.\n",
    "\n",
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "### Resampling\n",
    "\n",
    "- **Oversample** minority class (e.g., using SMOTE or random oversampling).\n",
    "- **Undersample** majority class.\n",
    "\n",
    "### Class weighting\n",
    "\n",
    "- Pass `class_weight='balanced'` to some models (e.g., logistic regression, random forest).\n",
    "- Adjusts loss function to penalize mistakes on minority class more.\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "- Use metrics that highlight minority class performance:\n",
    "  - Precision, recall\n",
    "  - F1-score\n",
    "  - ROC-AUC\n",
    "  - PR-AUC\n",
    "\n",
    "---\n",
    "\n",
    "## 🥷 Key points\n",
    "\n",
    "> \"Accuracy alone is a trap on imbalanced data. You need to focus on recall & precision to understand real-world clinical utility.\"\n"
   ],
   "id": "aa6716613fc65350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:30:24.740792Z",
     "start_time": "2025-07-01T23:30:24.409734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dummy example\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train_scaled[['gene_expression']], y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled[['gene_expression']])\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "405a9efebef29a4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisgaughan/anaconda3/envs/docker_ml_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chrisgaughan/anaconda3/envs/docker_ml_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chrisgaughan/anaconda3/envs/docker_ml_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dummy Data-Set is all well and good, but let's chack out a sample Breast Cancer data from `scikit-learn`",
   "id": "20d243f89e6e7fda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:36:37.537078Z",
     "start_time": "2025-07-01T23:36:37.356181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['target'].value_counts())\n"
   ],
   "id": "fb411407b723415",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Class distribution:\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:38:43.541547Z",
     "start_time": "2025-07-01T23:38:43.491707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# X and y\n",
    "X = df[data.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=5000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "id": "cbfd53b725e8ae7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94        42\n",
      "           1       0.99      0.94      0.96        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Experimental Plan: Mess Up the Pipeline to See Why It Matters\n",
    "\n",
    "## 💣 Hypothesis\n",
    "\n",
    "If we **don't stratify** during train/test split, or if we **fit the scaler on the whole data before splitting**, we might:\n",
    "\n",
    "- Cause **data leakage** (test info leaks into train).\n",
    "- End up with **skewed class distributions** in train/test.\n",
    "- Get **inflated or misleading model performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## Controlled experiment\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Use the scikit-learn breast cancer dataset.\n",
    "- Two experimental \"mistakes\":\n",
    "\n",
    "#### Mistake 1: No stratify during split\n",
    "\n",
    "- Without stratify, we risk imbalanced splits and underrepresenting classes.\n",
    "\n",
    "#### ⚠️ Mistake 2: Fit scaler on entire data before split\n",
    "\n",
    "- By fitting scaler on all data before splitting, we \"peek\" at test set statistics.\n",
    "\n",
    "---\n",
    "\n",
    "## Expected outcome\n",
    "\n",
    "- Train and test distributions will be different (class balance off).\n",
    "- Model metrics may look artificially better or worse.\n",
    "- Final metrics won’t reflect true generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "- Show why proper preprocessing (fit on train only, transform test only, and use stratify) is critical to avoid misleading results in clinical ML studies.\n"
   ],
   "id": "f68d8229811bf690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T23:44:06.891108Z",
     "start_time": "2025-07-01T23:44:06.807265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# ☠️Mistake 1: NO stratify\n",
    "X = df[data.feature_names]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ☢️Mistake 2: Fit on ALL data BEFORE split\n",
    "bad_scaler = StandardScaler()\n",
    "bad_scaler.fit(X)  # ⧮ fitting on all data\n",
    "\n",
    "X_scaled = pd.DataFrame(bad_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "# Now split \"scaled data\" (already contaminated)\n",
    "X_train_scaled = X_scaled.loc[X_train.index]\n",
    "X_test_scaled = X_scaled.loc[X_test.index]\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=5000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\nClass balance (train):\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass balance (test):\")\n",
    "print(y_test.value_counts(normalize=True))\n"
   ],
   "id": "da2bbb4b8a7e715b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        43\n",
      "           1       0.99      0.99      0.99        71\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n",
      "\n",
      "Class balance (train):\n",
      "target\n",
      "1    0.628571\n",
      "0    0.371429\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class balance (test):\n",
      "target\n",
      "1    0.622807\n",
      "0    0.377193\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lessons Learned from Messing Up the Pipeline\n",
    "\n",
    "## What we did wrong\n",
    "\n",
    "- **Did not use `stratify`** → Risks unbalanced train/test splits.\n",
    "- **Fit scaler on entire data before split** → Caused data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Consequences\n",
    "\n",
    "- Inflated precision, recall, and F1-score due to seeing test data prematurely.\n",
    "- Misleading evaluation metrics, especially dangerous in healthcare.\n",
    "\n",
    "---\n",
    "\n",
    "## Correct approach\n",
    "\n",
    "- Always **split first**, using `stratify=y` to maintain class balance.\n",
    "- Then **fit scaler only on training data**, transform both train and test after.\n",
    "- Use evaluation metrics beyond accuracy (e.g., recall, F1) to understand real-world performance.\n",
    "\n",
    "---\n",
    "\n",
    "> \"Fit on train, transform on train, transform on test — never fit on test.\"\n"
   ],
   "id": "6316bbc4648fe775"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🚨 What is Data Leakage?\n",
    "\n",
    "**Data leakage** occurs when information from outside the training dataset is used to create the model. This artificially inflates performance metrics during development, but the model fails in production because that information won't be available for new, unseen data.\n",
    "\n",
    "**The core problem:** Your model learns patterns that won't exist in real-world deployment."
   ],
   "id": "edc97b091730bfe0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Types of Leakage in Clinical ML\n",
    "\n",
    "### 1. **Scaling Leakage** (what we're preventing)\n",
    "```python\n",
    "# ❌ WRONG - Leakage!\n",
    "scaler.fit(X)  # Fits on ALL data including test\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# ✅ CORRECT - No leakage\n",
    "X_train, X_test = train_test_split(X)\n",
    "scaler.fit(X_train)  # Only learns from training data\n",
    "```\n",
    "\n",
    "**Why this matters:** When you fit the scaler on all data, the scaler learns the mean and standard deviation of the test set. Your model indirectly \"knows\" something about the test data's distribution.\n",
    "\n",
    "### 2. **Feature Engineering Leakage**\n",
    "```python\n",
    "# ❌ WRONG\n",
    "df['gene_zscore'] = (df['gene_expression'] - df['gene_expression'].mean()) / df['gene_expression'].std()\n",
    "X_train, X_test = train_test_split(df)\n",
    "\n",
    "# ✅ CORRECT\n",
    "X_train, X_test = train_test_split(df)\n",
    "train_mean = X_train['gene_expression'].mean()\n",
    "train_std = X_train['gene_expression'].std()\n",
    "X_train['gene_zscore'] = (X_train['gene_expression'] - train_mean) / train_std\n",
    "X_test['gene_zscore'] = (X_test['gene_expression'] - train_mean) / train_std\n",
    "```\n",
    "\n",
    "### 3. **Target Leakage** (especially dangerous in clinical data)\n",
    "```python\n",
    "# ❌ WRONG - Using future information\n",
    "# Example: predicting treatment response, but including\n",
    "# \"days_until_response\" as a feature\n",
    "# This info wouldn't exist at prediction time!\n",
    "\n",
    "X = df[['gene_expression', 'age', 'days_until_response']]  # ❌\n",
    "\n",
    "# ✅ CORRECT - Only use info available at prediction time\n",
    "X = df[['gene_expression', 'age', 'baseline_symptoms']]  # ✅\n",
    "```"
   ],
   "id": "5449312a07ce3db2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🕰️ The Time Machine Analogy\n",
    "\n",
    "Imagine you're developing a model in 2024 to predict patient outcomes:\n",
    "\n",
    "**Data leakage is like having a time machine:**\n",
    "- You travel to 2025, see which patients responded to treatment\n",
    "- You come back to 2024 and use subtle hints from that future knowledge\n",
    "- Your model looks amazing in testing!\n",
    "- But when you deploy it in the *real* 2025, it fails because it doesn't have the time machine anymore\n",
    "\n",
    "**In our scaling example:**\n",
    "- Fitting scaler on all data = using the time machine to peek at test patients\n",
    "- The scaler learns \"future\" statistics (test set mean/std)\n",
    "- Model gets subtle advantages it won't have on truly new patients"
   ],
   "id": "1eadfb2dfa44467c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🏥 Why Leakage is Especially Dangerous in Clinical ML\n",
    "\n",
    "In healthcare and bioinformatics, data leakage can have serious consequences:\n",
    "\n",
    "1. **Regulatory Failure**\n",
    "   - FDA/regulatory bodies will reject models with leakage\n",
    "   - Cannot claim true prospective validation\n",
    "\n",
    "2. **Patient Harm**\n",
    "   - Model performs well in testing (90% accuracy!)\n",
    "   - Deployed model performs poorly (60% accuracy)\n",
    "   - Wrong treatments given, delayed diagnoses\n",
    "\n",
    "3. **Resource Waste**\n",
    "   - Clinical trials designed around flawed models\n",
    "   - Expensive sequencing/tests ordered based on bad predictions\n",
    "   - Time and funding lost\n",
    "\n",
    "4. **Publication Retraction**\n",
    "   - Many papers retracted due to leakage in ML pipelines\n",
    "   - Damages scientific credibility\n",
    "\n",
    "**Real example:** A gene expression model claimed 95% accuracy predicting cancer subtype, but had fitted the normalizer on all data. Real-world accuracy was only 68%."
   ],
   "id": "9ca9612d8d7a5f1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 Leakage Visualization\n",
    "\n",
    "### Without Leakage (Correct):\n",
    "```\n",
    "[All Data]\n",
    "    ↓\n",
    "Split (stratify=y)\n",
    "    ↓\n",
    "[Train Data]  [Test Data]\n",
    "    ↓              ↓\n",
    "Fit Scaler        (Wait)\n",
    "    ↓              ↓\n",
    "Transform         Transform (using train stats)\n",
    "    ↓              ↓\n",
    "Train Model       Evaluate\n",
    "```\n",
    "\n",
    "### With Leakage (Wrong):\n",
    "```\n",
    "[All Data]\n",
    "    ↓\n",
    "Fit Scaler  ← 🚨 Test data statistics leak here!\n",
    "    ↓\n",
    "Transform All\n",
    "    ↓\n",
    "Split\n",
    "    ↓\n",
    "[Train Data]  [Test Data]\n",
    "    ↓              ↓\n",
    "Train Model       Evaluate (Falsely optimistic!)\n",
    "```"
   ],
   "id": "7bc1c83b9f752156"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Quantifying the Leakage Impact\n",
    "\n",
    "**What happened in our \"bad\" pipeline:**\n",
    "\n",
    "1. Test set had 114 samples with certain statistical properties\n",
    "2. When we fit the scaler on all 569 samples, it learned a mean that incorporated those 114 test samples\n",
    "3. The model training indirectly benefited from knowing the test distribution\n",
    "4. Metrics looked artificially good (98% F1 vs 96% in correct pipeline)\n",
    "\n",
    "**The 2% difference might seem small, but:**\n",
    "- In a study of 1000 patients, that's 20 misclassified cases\n",
    "- Could be life-or-death decisions in cancer diagnosis\n",
    "- Compounds with other modeling choices\n",
    "- Won't replicate in prospective validation"
   ],
   "id": "56f475c478ce5b3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:09:09.128662Z",
     "start_time": "2025-10-18T08:09:07.068062Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandas-stubs==2.3.2.250926",
   "id": "9c7247986e0629c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-stubs==2.3.2.250926\r\n",
      "  Downloading pandas_stubs-2.3.2.250926-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23.5 in /opt/conda/envs/dataengineering_env/lib/python3.10/site-packages (from pandas-stubs==2.3.2.250926) (2.2.6)\r\n",
      "Collecting types-pytz>=2022.1.1 (from pandas-stubs==2.3.2.250926)\r\n",
      "  Downloading types_pytz-2025.2.0.20250809-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Downloading pandas_stubs-2.3.2.250926-py3-none-any.whl (159 kB)\r\n",
      "Downloading types_pytz-2025.2.0.20250809-py3-none-any.whl (10 kB)\r\n",
      "Installing collected packages: types-pytz, pandas-stubs\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [pandas-stubs]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed pandas-stubs-2.3.2.250926 types-pytz-2025.2.0.20250809\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ✅ CORRECT approach:\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Learn mean & std from TRAINING data only\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)  # Scale train data\n",
    "X_test_scaled = scaler.transform(X_test)    # Scale test data with TRAIN stats\n",
    "```"
   ],
   "id": "9bbf15f398a7bc31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:14:23.069532Z",
     "start_time": "2025-10-18T08:14:20.851380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Load the data\n",
    "# ============================================\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"\\nFirst few rows of features:\")\n",
    "print(X.head())\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Split FIRST (with stratify)\n",
    "# ============================================\n",
    "# 🔑 KEY: Split happens BEFORE any scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,      # Maintains class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AFTER SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Check original statistics BEFORE scaling\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEFORE SCALING - Original Statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train 'mean radius' - mean: {X_train['mean radius'].mean():.2f}, std: {X_train['mean radius'].std():.2f}\")\n",
    "print(f\"Test 'mean radius' - mean: {X_test['mean radius'].mean():.2f}, std: {X_test['mean radius'].std():.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Fit scaler ONLY on training data\n",
    "# ============================================\n",
    "# 🔑 KEY: Scaler only learns from X_train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # ✅ Only fit on training data\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCALER LEARNED FROM TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Scaler learned mean for 'mean radius': {scaler.mean_[0]:.2f}\")\n",
    "print(f\"Scaler learned std for 'mean radius': {scaler.scale_[0]:.2f}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Transform BOTH train and test\n",
    "# ============================================\n",
    "# 🔑 KEY: Use the SAME scaler (with train stats) for both\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train),\n",
    "    columns=X.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AFTER SCALING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train 'mean radius' - mean: {X_train_scaled['mean radius'].mean():.6f}, std: {X_train_scaled['mean radius'].std():.2f}\")\n",
    "print(f\"Test 'mean radius' - mean: {X_test_scaled['mean radius'].mean():.6f}, std: {X_test_scaled['mean radius'].std():.2f}\")\n",
    "print(\"\\nNote: Train mean ≈ 0, std ≈ 1 (exact)\")\n",
    "print(\"      Test mean ≈ 0, std ≈ 1 (approximate, uses train's stats)\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Train model on scaled training data\n",
    "# ============================================\n",
    "model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    max_iter=5000\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: Evaluate on scaled test data\n",
    "# ============================================\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE (NO LEAKAGE)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ SUMMARY: CORRECT PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. Split data first (with stratify)\")\n",
    "print(\"2. Fit scaler on X_train only\")\n",
    "print(\"3. Transform X_train using fitted scaler\")\n",
    "print(\"4. Transform X_test using SAME fitted scaler (train stats)\")\n",
    "print(\"5. Train model on X_train_scaled\")\n",
    "print(\"6. Evaluate on X_test_scaled\")\n",
    "print(\"\\n🎯 No data leakage - model never saw test statistics!\")"
   ],
   "id": "ff45eacf0b2cfd66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ORIGINAL DATA\n",
      "============================================================\n",
      "Total samples: 569\n",
      "\n",
      "First few rows of features:\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                 0.07871  ...         25.38          17.33           184.60   \n",
      "1                 0.05667  ...         24.99          23.41           158.80   \n",
      "2                 0.05999  ...         23.57          25.53           152.50   \n",
      "3                 0.09744  ...         14.91          26.50            98.87   \n",
      "4                 0.05883  ...         22.54          16.67           152.20   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Class distribution:\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "AFTER SPLITTING\n",
      "============================================================\n",
      "Training samples: 455\n",
      "Test samples: 114\n",
      "\n",
      "Train class distribution:\n",
      "target\n",
      "1    0.626374\n",
      "0    0.373626\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "target\n",
      "1    0.631579\n",
      "0    0.368421\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "============================================================\n",
      "BEFORE SCALING - Original Statistics\n",
      "============================================================\n",
      "Train 'mean radius' - mean: 14.07, std: 3.50\n",
      "Test 'mean radius' - mean: 14.37, std: 3.63\n",
      "\n",
      "============================================================\n",
      "SCALER LEARNED FROM TRAINING DATA\n",
      "============================================================\n",
      "Scaler learned mean for 'mean radius': 14.07\n",
      "Scaler learned std for 'mean radius': 3.50\n",
      "\n",
      "============================================================\n",
      "AFTER SCALING\n",
      "============================================================\n",
      "Train 'mean radius' - mean: -0.000000, std: 1.00\n",
      "Test 'mean radius' - mean: 0.085785, std: 1.04\n",
      "\n",
      "Note: Train mean ≈ 0, std ≈ 1 (exact)\n",
      "      Test mean ≈ 0, std ≈ 1 (approximate, uses train's stats)\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE (NO LEAKAGE)\n",
      "============================================================\n",
      "Accuracy: 0.956\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94        42\n",
      "           1       0.99      0.94      0.96        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "============================================================\n",
      "✅ SUMMARY: CORRECT PIPELINE\n",
      "============================================================\n",
      "1. Split data first (with stratify)\n",
      "2. Fit scaler on X_train only\n",
      "3. Transform X_train using fitted scaler\n",
      "4. Transform X_test using SAME fitted scaler (train stats)\n",
      "5. Train model on X_train_scaled\n",
      "6. Evaluate on X_test_scaled\n",
      "\n",
      "🎯 No data leakage - model never saw test statistics!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ❌ WORST: Leakage + No stratify\n",
    "scaler.fit(X)  # Leakage!\n",
    "X_train, X_test = train_test_split(X)  # Imbalanced splits possible\n",
    "\n",
    "# ⚠️ BETTER: No leakage, but might have imbalanced splits\n",
    "X_train, X_test = train_test_split(X, y)\n",
    "scaler.fit(X_train)  # No leakage!\n",
    "# But train/test might have different class distributions\n",
    "\n",
    "# ✅ BEST: No leakage + Balanced splits\n",
    "X_train, X_test = train_test_split(X, y, stratify=y)\n",
    "scaler.fit(X_train)  # No leakage + balanced classes!\n",
    "```\n",
    "\n",
    "## In Clinical/Bioinformatics Context\n",
    "\n",
    "**Without stratify:**\n",
    "```\n",
    "Original: 90% non-responders, 10% responders\n",
    "Train: 92% non-responders, 8% responders  ← Model undertrained on responders\n",
    "Test: 85% non-responders, 15% responders  ← Evaluation skewed"
   ],
   "id": "4f1175f494b6c991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What Stratify Does\n",
    "stratify ensures your train and test sets have the same class proportions as the original data:"
   ],
   "id": "fa2c3ae8f8173836"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:31:53.092983Z",
     "start_time": "2025-10-18T08:31:53.014653Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Original data:\n",
    "- Class 0: 70%- Class 1: 30%\n",
    "\n",
    "# WITHOUT stratify - might get unlucky:\n",
    "`X_train, X_test = train_test_split(X, y, test_size=0.2)`\n",
    "Train might be: 65% class 0, 35% class 1\n",
    "Test might be:  80% class 0, 20% class 1  ← Imbalanced!\n",
    "\n",
    "WITH stratify - guaranteed balance:\n",
    "`X_train, X_test = train_test_split(X, y, test_size=0.2, stratify=y)`\n",
    "Train: 70% class 0, 30% class 1\n",
    "Test:  70% class 0, 30% class 1  ← Same as original!"
   ],
   "id": "aaa9420ca12f24b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a8a344047e20cef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
